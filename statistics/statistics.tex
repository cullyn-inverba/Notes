\documentclass[12pt,a4paper]{article}
\usepackage{inverba}

\newcommand{\userName}{Cullyn Newman} 
\newcommand{\class}{Cohen} 
\newcommand{\institution}{Udemy} 
\newcommand{\theTitle}{\color{r-Sun}Statistics}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\cleardoublepage
\fancyhead{}
\fancyhead[R]{\hyperlink{home}{\nouppercase\leftmark}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begingroup
\clearpage
\section{Data}\phantomsection
\subsection{Data Basics}
\begin{itemize}
    \item Frequent types of data in statistics:
        \begin{itemize}
            \item \textbf{Interval}: numeric scale with meaningful intervals, e.g. temperature in celsius.
            \item \textbf{Ratio}: numeric but with a meaningful zero, e.g. height.
            \item \textbf{Discrete}: numeric with with no arbitrary precision, e.g. population.
            \item \textbf{Ordinal}: sortable and discrete, e.g. education level.
            \item \textbf{Nominal}: non-sortable and discrete, e.g. genre.
        \end{itemize}
    \item \textbf{Sample data}: Data from \textit{some} members of a group.
    \item \textbf{Population data}: Data from \textit{all} members of a group.
    \item Sample population sometimes uses hat notation, e.g. \(\hat{\beta},~\hat{\sigma}\), or other slight ambiguities. Sample data is used more often than population in statistics.
\end{itemize}

\subsection{Visualizing Data}
\begin{itemize}
    \item \textbf{Bar plots}: used to represnet {\color{o-Sun}categorical} (nominal and ordinal) and {\color{o-Sun}discrete numerical} data.
    \item \textbf{Box plots}: collection of a data that is split into separate quartiles (the box) and data min/max points (whiskers) in order to illustrate {\color{o-Sun}overall distribution} of data and its potential outliers (often denoted by **). 
    \item \textbf{Histograms}: similar to bar plots, but with binned continuous data on the x-axis. {\color{o-Sun}Shape} and {\color{o-Sun}order} is meaningful.
        \begin{itemize}
            \item Histograms of \textbf{counts}: 
                \begin{itemize}
                    \item Often more meaningful interpretation of raw data.
                    \item Difficult to compare across datasets.
                    \item Does not need to sum up to 1.
                    \item Usually better for {\color{o-Sun}qualitative} inspection.
                \end{itemize}
            \item Histograms of \textbf{proportion}:
                \begin{itemize}
                    \item Can be more difficult to relate to raw data.
                    \item Easier to compare across datasets.
                    \item Illustrates proportion of dataset.
                    \item Usually better for {\color{o-Sun}quantitative} analysis.
                \end{itemize}
        \end{itemize}
    \item Translating from counts to proportions: \(bin_i = 100\,(bin_i \,/\, sum(bins))\)
    \item \textbf{Pie charts}: representation of nominal, ordinal, or discrete data that must sum up to 1.
\end{itemize}

\subsection{Visualizing Data: Revisited (Post-Chapter 2)}
\begin{itemize}
    \item Determining {\color{o-Sun}number of bins} for historgrams: 
        \begin{itemize}
            \item Number of bins (\(k\)) can be specified directly of calculated from width of bins \(h\):
                \begin{itemize}
                    \item \(k = \left\lceil \dfrac{\Delta x}{h} \right\rceil \)
                    \item \(\lceil \rceil \) represnets the ceiling, our rounding up to nearest int.
                \end{itemize}
            \item \textbf{Sturges} guideline: \(k=\left\lceil \log2(n)\right\rceil +1 \)
                \begin{itemize}
                    \item Advantage: relates to the data count.
                \end{itemize}
            \item \textbf{Freedman-Diaconis}: \(h = 2\dfrac{IQR}{\sqrt[3]{n}}\)
                \begin{itemize}
                    \item Advantage: relates to both the data count and data spread.
                \end{itemize}
            \item Arbitrary choices, or other methods, often are decent enough and easier to implement, though Freedman-Diaconis is usually the best choice.
        \end{itemize}
    \item \textbf{Violin plots}: a {\color{o-Sun}rotated} and {\color{o-Sun}mirrored} historgram. 
        \begin{itemize}
            \item IQR, mean, median, and distribution can all be easily represented.
            \item Usually symmetric, but can comopare two similar datasets asymmetrically.
            \item Swarm plots are similar, except you can see individual data points.
        \end{itemize}
\end{itemize}

%\endgroup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begingroup
\clearpage
\section{Describing Statistics}\phantomsection
\subsection{Descriptive vs. Inferential}
\begin{itemize}
    \item \textbf{Descriptive}:
        \begin{itemize}
            \item The point is to obtain individual numbers that describe a dataset.
            \item Mean, median, mode, variance, kurtosis, skew, distribution, spectrum.
            \item No relation to population; no generalization to other datasets of groups.
        \end{itemize}
    \item \textbf{Inferential}: 
        \begin{itemize}
            \item Use features of sample data set to make generalizations about a population.
            \item P-value, T/F/chi-square value.
            \item Confidence intervals.
            \item Hypothesis testing.
        \end{itemize}
\end{itemize}

\subsection{Accuracy, Precision, Resolution}
\begin{itemize}
    \item \textbf{Accuracy}: the relationship between measurement and the actual truth. Inversely related to {\color{o-Sun}bias}.
    \item \textbf{Precision}: the certainty of each measurement. Inversely related to {\color{o-Sun}variance}.
    \item \textbf{Resolution}: the number of data points per unit measurement.
\end{itemize}

\subsection{Data Distribution}
\begin{itemize}
    \item \textbf{Data Distribution}: a function that lists values or intervals of data, and how often each value occurs. 
    \item Common distributions include power-law, gaussian (bell curve), t, F, and Chi-squared.
    \item Most statistical procedures are based on assumptions about distributions.
    \item Data distributions provide insights into nature and often used to model physical and biological systems.
\end{itemize}

\subsection{Measures of Central Tendency}
\begin{itemize}
    \item \textbf{Central tendency}: the center of typical value for a probability distribution. 
    \item Common measures of central tendency: {\color{o-Sun}mean, median, mode}.
    \item {\color{o-Sun}Mean}, aka average or arithmetic mean: 
    \begin{itemize}
        \item Formula: {\color{o-Sun}\(\bar{x}=n^{-1}\sum{x_i}\)}.
        \item Alternate notations for mean: \(\mu,\,\mu_x\).
        \item The mean is most suitable for normally distributed interval and ratio data.
        \item Discrete and ordinal data can be useful, but must be carefully interpreted.
    \end{itemize}
    \item {\color{o-Sun}Median}:
        \begin{itemize}
            \item {\color{o-Sun}\(x_i,~i=\dfrac{n+1}{2}\)}
            \item Most suitable for unimodal distributed interval and ratio data.
        \end{itemize}
    \item {\color{o-Sun}Mode}: the most common value that is suitable for any distribution and data type, though mostly used for nominal.
\end{itemize}

\subsection{Measures of Disperion}
\begin{itemize}
    \item \textbf{Dispersion}: also called variability, scatter, or spread; a single number that describes how dispersed the data is around the central tendency. 
    \item Main measures of dispersion: {\color{o-Sun}standard deviation} and {\color{o-Sun}variance}.
    \item {\color{o-Sun}Variance}: indicates dispersion around the mean.
        \begin{itemize}
            \item Formula: {\color{o-Sun}\(\sigma^2=\dfrac{1}{n-1}\sum(x_i-\bar{x})^2\)}
            \item Suitable for any distribution. 
            \item Works best with numerical data, or ordinal data with a mean.
            \item Taking the absolute value instead of the square of the mean difference results in the \textit{mean absolute difference (MAD)}.
            \item Squaring emphasizes large values; better for optimization; closer to euclidean distance; is the second "moment"; better link to least-squares regression; and more.
            \item MAd is robust to outliers, though less commonly used. 
            \item Dividing by \(N-1\) is for sample variance, while \(N\) is for population.
        \end{itemize}
    \item {\color{o-Sun}Standard deviation}: simply the square root of variance. 
    \item Knowing the standard deivation gives you variance and vice versa. Variance is more useful mathematically, while standard deviation has convenience of being expressed in units of the original variable.
    \item There other related measures such as \textit{Fano factor} and \textit{Coefficient of variation}, which are normalized measures of variability. Sensible only for datasets with positive values. 
    \item \textit{Fano factor}: \(F = \dfrac{\sigma^2}{\mu}\); variance divided by the mean.
    \item \textit{Coefficient of variation}: \(CV= \dfrac{\sigma}{\mu}\); standard deviation divided by the mean. 
\end{itemize}

\subsection{Interquartile Range and QQ Plots}
\begin{itemize}
    \item Each half of the data made by the median can be divided further by taking the median again, resulting in 3 boundary points, or {\color{o-Sun}quartiles}
    \item Quartile 1 is the "left"; quatile 2 is the middle, or "global median", and quartile 3 is the right.
    \item \textbf{Interquartile range (IQR)}: the range between quartile 1 and 2 that represnets 50\% of the data.
    \item \textit{Revisiting box plots}: IQR is represented by the box of the plot. 
    \item \textbf{QQ plots}: aka quantile-quantile plots; a diagnostic scatter plot that compares two probability distributions by plotting their quantiles against each other in order to determin if it comes from a normal distribution.
\end{itemize}

\subsection{Statistical Moments}
\begin{itemize}
    \item Unstandardized statiscal moments:
        \begin{itemize}
            \item General formula: {\color{o-Sun}\(m_k = n^{-1}\sum(x_i-\bar{x})^k\)}
            \item \textit{First moment}: the {\color{o-Sun}mean}, with a \(k\) value of 1.
            \item \textit{Second moment}: the {\color{o-Sun}variance}, with \(k\) value of 1.
            \item Further moments are increments of k.
        \end{itemize}
    \item Standardized statiscal moments:
        \begin{itemize}
            \item Third and fourth moments are standardized with additional variance terms, {\color{o-Sun}\((n\sigma^k)^{-1}\)} insted of just \(n\).
            \item \textbf{Skewness}: the \textit{third moment}; dispersion asymmetry around the mean.
                \begin{itemize}
                    \item {\color{pos}positive skew}; the range of outliers pulled to right.
                    \item {\color{neg}negative skew}; the range of outliers pulled to the left. 
                \end{itemize}
            \item \textbf{Kurtosis}: the \textit{fourth moment}; the length of the distribution from the mean, heavy-tailed or light-tailed, relative to the normal distribution.
                \begin{itemize}
                    \item Data with {\color{o-Sun}low} kurtosis have light tails, or {\color{o-Sun}lack of outliers}.
                    \item Data with {\color{o-Sun}high} kurtosis have heavy tails, or {\color{o-Sun}more outliers}.
                \end{itemize}
        \end{itemize}
    \item There are further moments, but generally lack significance.
    \item \textbf{Shannon entropy}: entropy related to information processing that represnets the average level of information/uncertainty inherent in a variable possbile outcomes.
        \begin{itemize}
            \item Surprising events convey more information.
            \item Formula: {\color{o-Sun}\(H = -\sum p(x_i)\log_2(p(x_i))\)}
            \item x = data values, p = probability.
            \item Used for nominal, ordinal, or discrete data.
            \item Interval or ratio data must be coverted to discrete by binning; entropy is affected by bin width and number.
            \item {\color{pos}High entropy} means the dataset has {\color{pos}high variability}.
            \item {\color{neg}Low entropy} means most values repeat and offer {\color{neg}redundant} information.
            \item Entropy is related to variance, though it is 
            {\color{o-Sun}nonlinear} and makes {\color{o-Sun}no assumptions} about distribution.
            \item \(\log_2\) entropy gives {\color{o-Sun}bit} based units.
            \item \(\ln\) entropy accross datasets of consists units gives {\color{o-Sun}nat} based units.
        \end{itemize}
\end{itemize}


%\endgroup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}