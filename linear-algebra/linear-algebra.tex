\documentclass[12pt,a4paper]{article}
\usepackage{inverba}

\newcommand{\userName}{Cullyn Newman} 
\newcommand{\class}{Cohen} 
\newcommand{\institution}{Udemy} 
\newcommand{\theTitle}{\color{r-Sun}Linear Algebra}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\cleardoublepage
\fancyhead{}
\fancyhead[R]{\hyperlink{home}{\nouppercase\leftmark}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begingroup
\clearpage
\section{Vectors}\phantomsection
\subsection{Interpretations of Vectors}
\begin{itemize}
    \item \textbf{Vector}: an ordered list of numbers. 
    \item Possible notations: \(\vec{v}=\bm{v}\) are most common.
    \item \textbf{Dimensionality}: the number of the elements in a vector.
    \item \textbf{Geometric vector}: an object with a magnitude and direction.
    \item {\color{o-Sun}Standard position}: when the vector beings at the origin.
    \item Vectors must have same dimensionality for addition and subtraction.
    \item \textbf{Unit vector}: a vector with a {\color{o-Sun}norm} (length) of 1. Notation: \(\hat{u}=\dfrac{\bm{u}}{|\bm{u}|}\) 
\end{itemize}
\subsection{Vector Multiplication}
\begin{itemize}
    \item \textbf{Scalar}: scales each element in a vector, does not change direction. Generally represented with greek letters.
    \item \textbf{Dot product}: a single number that provides information about the relationship between two vectors. Must have {\color{o-Sun}same dimensionality}.
    \item Notation for dot product: \(\bm{a}\cdot \bm{b} = \bm{a}^T\bm{b} = \left\langle \bm{ab} \right\rangle = \sum a_i b_i\)
    \item \textit{Algebraic} dot product properties:
        \begin{itemize}
            \item {\color{false}\textit{Associative:}} \(\bm{a}^T(\bm{b}^T\bm{c})~{\color{false}\neq}~(\bm{a}^T\bm{b})^T\bm{c}\)
            \item {\color{true}\textit{Distributive: }} \(\bm{a}^T(\bm{b}+\bm{c})~{\color{true}=}~\bm{a}^T\bm{b} + \bm{a}^T\bm{c}\)
            \item  {\color{true}\textit{Commutative:}} \(\bm{a}^T\bm{b} ~{\color{true}=}~\bm{b}^T\bm{a}\)
            \item Vector magnitude/length: {\color{o-Sun}\(\left\lVert \bm{v}\right\rVert = \sqrt{\bm{v}^T\bm{v}}\)}
        \end{itemize}
    \item \textit{Geometric} dot product properties:
        \begin{itemize}
            \item Magnitudes of vectors scaled by angle between them. i.e. {\color{o-Sun}\(\vec{a} = |a||b|\cos(\theta_{ab})\)}
            \item Geometric and algebraic are really the same. The above equation can be rewritten as the algebraic vector length, i.e.  {\color{o-Sun}\(\bm{a}^T\bm{b} = \cos(\theta_{ab})|a||b|\)}
        \end{itemize}
    \item Dot product features based on \(\theta\):
        \begin{itemize}
            \item If {\color{pos}\(\cos(\theta) > 0\)} then {\color{pos}\(\alpha > 0\)}
            \item If {\color{neg}\(\cos(\theta) < 0\)} then {\color{neg}\(\alpha < 0\)}
            \item If \(\cos(\theta) = 0\) then \(\alpha = 0\); termed \textbf{Orthogonal}
            \item If \(\cos(\theta) = 1\) then \(\alpha = |a||b|\)
        \end{itemize}
    \item \textit{Hadamard vector multiplication}: elementwise multiplication of two vectors of equal dimensionality.
    \item \textbf{Outer product}: {\color{o-Sun}\(\bm{vw}^T = n \times m\)}; a matrix resulting from the product of vectors with dimensions \(n\) and \(m\).
    \item \textbf{Cross product}: defined only between {\color{o-Sun}two 3D} vectors; produces another 3D vector that is perpendicular to both original vectors, or {\color{o-Sun}normal}, to the plane containing them.
    \item \textit{Complex conjugate}: the inverse sign of imaginary componenet of a number.
    \item \textbf{Hermitian transpose}: or conjugate transpose, a transpose of a vector or matrix containing imaginary numbers using the complex conjugate. 
    \item Notation for Hermitian transpose on a matrix: \(\bm{M}^H\) or \(\bm{M}^*\)
\end{itemize}

\subsection{Vector Space}
\begin{itemize}
    \item \textbf{Dimension}: in linear algebra dimension represnets a {\color{o-Sun}new element} of infomation about a vector. Geometrically, each dimension represent a {\color{o-Sun}new direction}.
    \item \textbf{Fields}: a set of numbers on which a set of arithmetic are valid.
    \item Field notations:
        \begin{itemize}
            \item $\mathbb{R}$: real numbers.
            \item $\mathbb{C}$: complex numbers.
            \item $\mathbb{K}$: real complex numbers.
            \item $\mathbb{Z}$: integers.
            \item $\mathbb{N}$: poisite integers.
            \item $\mathbb{Q}$: rational numbers.
            \item Dimensionality in fields is written with superscripts, e.g. \(\mathbb{R}^{\color{o-Sun}N}\)
        \end{itemize}
    \item \textbf{Subspace}: The set of all vectors that can be created by a linear combination of some vectors or a set of vectors, i.e. {\color{o-Sun}\(\lambda\bm{v},~\lambda\in\mathbb{R}\)}
    \begin{itemize}
        \item More formal definition: a vector subspace (\(V\)) must be closed under addition and scalar multiplication, and must contain the zero vector.
        \item {\color{o-Sun}\(\forall\bm{v}, \bm{w} \in V;~ \forall\lambda,\alpha\in\mathbb{R};~ \lambda\bm{v}+\alpha\bm{w}\in V\)}
        \item Geometric: all possible scaled versions of vector produces a line for a single vector, and a 2d plane between two linear independent vectors.
    \end{itemize}
    \item \textbf{Subset}: a set of points that satisfies some conditions; doesn't need to include toe origin, doesn't need to be closed, and can have boundaries.
    \item \textbf{Span}: all possible linear combinations of vectors within a subspace. 
    \item \textbf{Linear Independence}: a property of an entire set of vectors that is true only if no vector in the set can be written as a linear combination of the others.
        \begin{itemize}
            \item Geometric perspective: a set of M vectors is independent if each vector points in a geometric dimension not reachable using other vectors in the set.
            \item Any set of \({\color{pos}M>N}\) vectors in \(\mathbb{R}^N\) is {\color{o-Sun}dependent}.
            \item Any set of \({\color{neg}M\leq N}\) vectors in \(\mathbb{R}^N\) \textit{could be} {\color{o-Sun}independent}.
        \end{itemize}
    \item \textbf{Basis}: a combination of span and independence. More formally: if every element of a vector space \(V\) over a field is a {\color{o-Sun}linearly independent subset} of \(V\) that {\color{o-Sun}spans} \(V\).
\end{itemize}
%\endgroup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begingroup
\clearpage
\section{Introcution to Matrices}\phantomsection
\subsection{Terminology and Dimensionality}
\begin{itemize}
    \item Matrix notation: 
        \begin{itemize}
            \item \textbf{M} for reference to entire matrix.
            \item m\(_{M,N}\) represnets an individual element in a matrix.
            \item \textbf{Block matrix}: a matrix that includes matrices itself; useful for matrices with higher level structure and can have computational benifits. e.g:
            \begin{align*}
            M=
            \begin{bmatrix}
                {\color{p-Haze}\bm{D}} & {\color{false}\bm{0}} \\
                {\color{true}\bm{1}}  & {\color{p-Haze}\bm{D}}
            \end{bmatrix} =
            \begin{bmatrix}
                {\color{p-Haze} 3} & {\color{p-Haze} 0} & {\color{false}\bm{0}} & {\color{false}\bm{0}} \\
                {\color{p-Haze} 0} & {\color{p-Haze} 4} & {\color{false}\bm{0}} & {\color{false}\bm{0}} \\
                {\color{true}\bm{1}} & {\color{true}\bm{1}} & {\color{p-Haze} 3} & {\color{p-Haze} 0} \\
                {\color{true}\bm{1}} & {\color{true}\bm{1}} & {\color{p-Haze} 0} & {\color{p-Haze} 4} 
            \end{bmatrix}
            \end{align*}
            \item \textbf{Diagonal}: the diagonal elements of a matrix going form top left to bottom right.
                \begin{itemize}
                    \item \textit{Off-diagonal}: elements not in the diagonal elements.
                    \item Works for nonsquare matrices.
                \end{itemize}
            \item Rows (M) first, then (N) columns when describing matrices. 
            \item \textbf{Dimensionality}: more open to interpretations:
                \begin{itemize}
                    \item \(\mathbb{R}^{MN}\): simply the product, or total number of elements.
                    \item \(\mathbb{R}^{M\times N}\): more explicit, not the product. \(M\times N\) could be different from \(N\times M\)
                    \item \(C(M)\in \mathbb{R}^M\): \textit{column space} that is the collection of column vectors.
                    \item \(C(M)\in \mathbb{R}^N\): \textit{row space} that is the collection of row vectors.
                    \item Ambiguity opens up flexibility, but makes terminology more context dependent.
                \end{itemize}
            \item \textbf{Tensor}: higher dimensional cubes, won't be discussed much in early linear algebra.
        \end{itemize}
    \item Types of common matrices:
        \begin{itemize}
            \item \textbf{Square}: \(M\times M\)
                \begin{itemize}
                    \item \textbf{Symmetric}: a square matrix that is symmetric across the diagonal.
                    \item \textbf{Skew-symmetric}: diagonal must be zero, and numbers must mirror signs across diagonal.
                    \item \textbf{Identity}: square matrix with 1s across the diagonal and zeros everywhere elses.
                    \begin{itemize}
                        \item Notation: \(I_x\).
                        \item x indicates size of matrix; no subscript means it is relevent size to matrix it is being applied to.
                    \end{itemize}
                \end{itemize}
            \item \textbf{Rectangular}: non-square, \(M\times N\)
            \item \textbf{Zero}: matrix consisting of only zeros.
            \item \textbf{Diagonal}: all off diagonal elements are zero.
                \begin{itemize}
                    \item Identity matrix is a special case of diagonal.
                \end{itemize}
            \item \textbf{Triangular}:
                \begin{itemize}
                    \item \textit{Upper}: all elements {\color{o-Sun}below} diagonal are zero.
                    \item \textit{Lower}: all elements {\color{o-Sun}above} diagonal are zero.
                \end{itemize}
            \item \textbf{Concatenated}: two matrices with same number of rows concatenated. Often a line is placed in the product matrix to indicate concatenation point.
        \end{itemize}
\end{itemize}

\subsection{Basic Matrix Arithmetic}
\begin{itemize}
    \item Addition/subtraction is defined only for two matrices with same number of elements.
        \begin{itemize}
            \item Simply a-dd/subtract corresponding elements.
            \item {\color{true}Commutative}: \(\textbf{A} + \textbf{B} = \textbf{B} + \textbf{A}\)
            \item {\color{true}Associative}: \(\mathbf{A} + (\mathbf{B}+\mathbf{C}) = (\mathbf{A}+\mathbf{B}) + \mathbf{C}\)
        \end{itemize}
    \item \textbf{Shifting}:
        \begin{itemize}
            \item Adding a scaled version of identity matrix.
            \item Formula: \(\textbf{A} + \lambda\bm{I} = \textbf{C}\)
            \item Geometrically infats matrix, pushing it towards a sphere.
            \item Tends to {\color{o-Sun}regularize} the matrix.
        \end{itemize}
    \item \textbf{Matrix-Scalar multiplication}:
        \begin{itemize}
            \item Elementwise multiplication by the scalar.
            \item {\color{true}Commutative}: \(\delta\textbf{MA}=\textbf{M}\delta\textbf{A}=\textbf{MA}\delta\)
            \item {\color{true}Distributive}: \(\delta(\textbf{MA}) = \delta\textbf{M} + \delta{\textbf{A}}\)
            \item Distributive nature makes it a linear opperation.
        \end{itemize}
    \item \textbf{Matrix transpose}:
        \begin{itemize}
            \item \((\textbf{A}^{M\times N})^T = \textbf{A}^{N\times M}\)
            \item First column becomes first row, and vice versa.
            \item \(\textbf{A}^{TT} = \textbf{A}\)
            \item \textit{Symmetric}: \(\textbf{A} = \textbf{A}^T\)
            \item \textit{Skew-symmetric}: \(\textbf{A} = \textbf{-A}^T\)
        \end{itemize}
    \item \textbf{Complex matrices}: similar to vectors, a matrix containing at least one non-zero imaginary component.
        \begin{itemize}
            \item Hermitian transpose only changes the sign (the conjugate) of the complex parts, not the real numbers.
        \end{itemize}
    \item \textbf{Diagonal and trace}: extracts the diagonal as a vector.
        \begin{itemize}
            \item Defined both for square and rectangular.
            \item Not the same as \textit{diagonalizing} a matrix.
            \item \textbf{Trace}: the sum of the vector extracted from a matrix and is defined only for square matrices.
            \item Diagonal formula: {\color{o-Sun}\(\textbf{v}_i = \textbf{A}_{i,i}~~ i = \{1,2,...,min(m,n)\}\)}
            \item Trace formula: {\color{o-Sun}\(tr(\textbf{A})= \sum \textbf{A}_{i, i}\)}
        \end{itemize}
    \item \textbf{Broadcasting}: expand, or repeat, a vector until traditional arithmetic is valid between a matrix and the vector.
\end{itemize}

%\endgroup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begingroup
\clearpage
\section{Matrix Multiplications}\phantomsection
\subsection{Standard Matrix Multiplication}
\begin{itemize}
    \item Order of multiplication often matters and is generally {\color{false}not communitive}.
        \begin{itemize}
            \item i.e. \textbf{AB} $\neq$ \textbf{BA}.
        \end{itemize}
    \item Rules for validity:
        \begin{itemize}
            \item Multiplication is {\color{o-Sun}valid} only when the {\color{o-Sun}inner dimensions} match.
            \item Resulting matrix {\color{o-Sun}size} corresponds to the {\color{o-Sun}outer dimensions}.
            \item i.e. \(\textbf{M}\times {\color{true}\textbf{N}} \cdot {\color{true}\textbf{N}}\times\textbf{K}= \textbf{M}\times\textbf{K}\) 
        \end{itemize}
    \item Methods of multiplication:
        \begin{itemize}
            \item \textbf{Element perspective}: an order collection of dot products; each element is a dot product between {\color{o-Sun}rows of the left} and {\color{o-Sun}columns of the right} matrix. 
            \item \textbf{Layer perspective}: outer products summed between columns of the left matrix and rows of the right matrix.
                \begin{itemize}
                    \item or the sum of \textit{rank 1 matrices}.
                \end{itemize}
            \item \textbf{Column perspective}: columns of the left matrix scaled then summed by the elements of the right matrix to make a column in the product matrix.
                \begin{itemize}
                    \item or a \textit{linear weighted combination} of the columns in the left, weighted by the columns in the right.
                \end{itemize}
            \item \textbf{Rows perspective}: rows of the left matrix scales the rows of right matrix then summed to make a row in the product matrix.
        \end{itemize}
\end{itemize}

\subsection{Diagonal Matrix Multiplication}
\begin{itemize}
    \item \textbf{A{\color{o-Sun}D}} \(\rightarrow\) Post-multiplying by a diagonal.
    \item \textbf{{\color{o-Sun}D}A} \(\rightarrow\) Pre-multiplying by a diagonal.
    \item Post-multiplication results in the dense matrix's columns being weighted by the diagonal elements, while pre-multiplation scales the rows.
\end{itemize}

\subsection{Order of Operations}
\begin{itemize}
    \item \((\textbf{LIVE})^T = \textbf{E}^T\textbf{V}^T\textbf{I}^T\textbf{L}^T\)
    \item Matrices multipied then transposed is equal to the same matrices transposed then multiplied in reverse direction.
\end{itemize}

\subsection{Matrix-Vector Multiplication}
\begin{itemize}
    \item The result is always vector when multiplying by a matrix by a vector.
    \item The product vector always has the same orientation as the intput vector.
    \item The length of the vector is determined by the length of the matrix. 
    \item \textbf{Aw} \(\rightarrow\) a weighted combination of the {\color{o-Sun}columns} of \textbf{A}.
    \item \textbf{w}\(^{T}\textbf{A} \rightarrow\) a weighted combination of the {\color{o-Sun}rows} of \textbf{A}.
    \item Symmetric matrices have the same result.
\end{itemize}

\subsection{Additive and Multiplicative Matrix Identities}
\begin{itemize}
    \item Pre or post multiplication of an identity matrix gives you the same matrix, i.e. \textbf{AI} = \textbf{IA} = \textbf{A}
        \begin{itemize}
            \item This is the {\color{o-Sun}multiplicative indentity}, which is often just called the identity matrix.
        \end{itemize}
    \item Adding a zeros matrix is the {\color{o-Sun}additive identity}, i.e. \textbf{A} + \textbf{0} = \textbf{A}
    \item Methods of creating symmetric matrices from non-symmetric matrices:
        \begin{itemize}
            \item Additive method: \(\textbf{S} = (\textbf{A}+\textbf{A}^T)/2\) 
                \begin{itemize}
                    \item Works only if \textbf{A} is a square matrix.
                \end{itemize}
            \item Multiplicative method: 
                \begin{itemize}
                    \item \(\textbf{A}^T\textbf{A}=\textbf{S}\) that is \(n\times n\)
                    \item \(\textbf{A}\textbf{A}^T=\textbf{S}\) that is \(m\times m\)
                    \item Always produces a {\color{o-Sun}symmetric square} matrix.
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Multiplication of Symmetric Matrices}
\begin{itemize}
    \item The product of symmetric matrices is not itself symmetric.
    \item A \(2\times2\) matrix with constant diagonals will produce symmetric matrix.
\end{itemize}
\newpage
\subsection{Frobenius Dot Product}
\begin{itemize}
    \item There are three ways to calculate the Frobenius dot product:
        \begin{enumerate}
            \item Hadamard multiplation then sum all elements.
                \begin{itemize}
                    \item \textbf{Hadamard multiplation}: element wise multiplation; valid only with matrices of equal size.
                    \begin{itemize}
                        \item A diagonal matrix multiplied with its self has the same product using Hadamard or standard matrix multiplation.
                    \end{itemize}
                \end{itemize}
            \item Vectorize both matrices then compute the vector dot product.
                \begin{itemize}
                    \item \textbf{Vectorizating a matrix}: creating a vector of matrix by concatenating column wise of a matrix.
                \end{itemize}
            \item Transpose and trace of the product.
                \begin{itemize}
                    \item Notation: \(\left\langle \textbf{A},\textbf{B}\right\rangle_F = tr(\textbf{A}^T\textbf{B}) \)
                    \item \textbf{Trace}: the sum of the elements in the main diagonal. 
                    \item The most efficient method of calculating the Frobenius dot product.
                \end{itemize}
        \end{enumerate}
\end{itemize}
%\endgroup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}